---
title: "Probabilistic alternative to 2 group t-test"
author: "Rasmus Hindstr√∂m"
date: last-modified
format:
    html:
        toc: true
        toc_float: true
        embed-resources: true
        code-fold: true
---

# 0. Summary

This report demonstrates the use and interpretation of a probabilistic alternative
to the two sample t-test. For a more expansive deep dive into the topic refer to
Matti Vuorre's blog post [How to Compare Two Groups with Robust Bayesian Estimation in R](https://vuorre.com/posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/).
Much of which is adapted within the context of microbiome research. 

A common task in microbiome research is to compare alpha diversity between
two or more groups. Often done with the t-test
and concluded as a binary "significant or non-significant" result, the more
rich inference provided by the probabilistic comparison will become apparent.
Alpha diversity is a measure of the within sample diversity, and it is often computed
as the number of observed species (richness), the Shannon index, or the Simpson index. 
Here we demostrate comparisons of the Shannon index which takes into account abundance
information, and is typically close to normally distributed.  

# 1. Comparison of two groups

Here a dataset from the `mia` package is used to demonstrate the comparison and 
interpretation differences between the two sample t-test (Welch t-test) and an
alternative probabilistic approach with Bayesian estimation using the `brms` package.

```{r}
#| label: load-libraries
#| code-summary: Load required libraries
#| echo: false
#| output: false 

library(mia)
library(dplyr)
library(brms)
library(bayesplot)
library(bayestestR)
library(ggplot2)
library(posterior)
library(patchwork)
```

```{r}
#| label: prepare-data
#| code-summary: prepare example data
#| 
data("peerj13075", package = "mia")
tse <- peerj13075

# Add Shannon index
tse <- addAlpha(
    tse,
    assay.type = "counts",
    index = "shannon"
)

# Construct data frame
df <- as.data.frame(colData(tse))
```

## 1.1. Welch t-test

Welch t-test makes frees the assumption of equal variances between the two groups.
Still retaining the assumption of normality.

In essance the t-test compares the means of two groups, and returns a p-value
indicating the probability of observing the data if the null hypothesis is true.
The null hypothesis states that there is no difference between the two groups.

```{r}
#| label: welch-t-test
#| code-summary: Perform Welch t-test

t.test(shannon ~ Diet, data = df, var.equal = FALSE)
```

The p-value suggests that there is no significant difference between the two groups.
We fail to reject the null hypothesis. 

## 1.2. Bayesian estimation

Approaching the same problem with Bayesian estimation using the `brms` package.

As Matti Vuorre (2017) points out, the t-test is only a linear model under the hood.
Extending this idea, we can formulate a probabilistic model and use `brms` to estimate
the parameters of interest. In this case the group means and variances.

$$y_{ik} \sim \mathcal{N}(\mu_k, \sigma_k^2)$$

$$ \mu_k = \beta_0 + \beta_k$$

$$ \sigma_k^2 = \gamma_0 + \gamma_k$$

```{r}
#| label: bayesian-estimation
#| code-summary: Perform Bayesian estimation

# Fit Bayesian model (default priors)
fit <- brm(
    formula = bf(
        shannon ~ Diet,
        sigma ~ Diet
    ),
    data = df,
    family = gaussian(),
    iter = 4000,
    chains = 4,
    cores = 4
)

summary(fit)
plot(fit)
pp_check(fit)
```

The model summary shows non-pathogenic Rhat values, indicating that the chains have converged.
Effective sample sizes are sufficiently high (> 1000) for each parameter.
Chains are well mixed in the traceplots.
The posterior predictive plot shows that the model is able to capture the observed data well enough.

The posterior distributions of the group means and standard deviations can be extracted for further plotting and interpretation.

```{r}
#| label: bayesian-estimation-plots
#| code-summary: Plot posterior distributions

draws <- as_draws_df(fit) 

mu_group1 <- draws$b_Intercept
mu_group2 <- draws$b_DietVeg + mu_group1
sigma_group1 <- exp(draws$b_sigma_Intercept)
sigma_group2 <- exp(draws$b_sigma_DietVeg + draws$b_sigma_Intercept)

plot_data <- data.frame(
    mu_group1 = mu_group1,
    mu_group2 = mu_group2,
    sigma_group1 = sigma_group1,
    sigma_group2 = sigma_group2
)

p1 <- ggplot(plot_data, aes(x = mu_group1)) +
    geom_density(fill = "orange", alpha = 0.7) +
    geom_density(aes(x = mu_group2), fill = "purple", alpha = 0.7) +
    labs(title = "Posterior Distributions of Group Means",
         x = "Shannon Index",
         y = "Density") +
    annotate("text", x = max(c(mu_group1, mu_group2)), y = Inf, 
             label = "Orange: Group 1\nPurple: Group 2", 
             vjust = 2, hjust = 1) +
    theme_minimal()

p1
```


## 1.3. Conclusions



>>> TODO: RESTRUCTURE <<<

# 1. Introduction

In microbiome research it is common to compute alpha diversity metrics and compare
them between two groups. Alpha diversity is a measure of the within sample diversity,
and it is often computed as the number of observed species (richness), the Shannon index,
or the Simpson index. Frequentist methods for comparing groups include
the t-test and the Wilcoxon rank-sum test. For comparision of more than two groups,
Kruskal-Wallis test is commonly used, along with ANOVA and PERMANOVA. 

However, these methods have their shortcomings, and rely on assumptions that are
often violated in practice when handling microbiome data (e.g. non-normality, 
heteroscedasticity, and sparsity). Probabilistic methods, provide a more flexible
framework able to handle these issues in a robust way. Yet, they are not widely
used in the field of microbiome research. Increasing awareness and showcasing
the advantages of probabilistic methods as an alternative contributes to 
laying the foundation for wider adoption within the field. 

## 1.1. Alpha Diversity and the Shannon index

Higher alpha diversity, the diversity within a sample, is often associated in studies with a healthy state. A microbiome with high diversity is thought to be more resilient to perturbations, and more capable of performing a wide range of functions. The simplest measure of alpha diversity is just the number of observed species, richness, but this does not take into account relative abundances or other information about the community. The Shannon index is one such diversity measure that takes into account the distribution of taxa, or more precicsely how evenly they are distrubuted. Put simply, more species distributed evenly results in a higher Shannon index. Less species, with uneven distribution leads to a low index. Shannon index has the feature of being typically close to normally distributed in datasets.

## 1.2. Two sample t-test

The two sample t-test is a statistical test used to compare the means of two groups. It is based on the assumption that the data is normally distributed, and that the variances of the two groups are equal. The t-test returns a p-value, which indicates the probability of observing the data if the null hypothesis is true. The null hypothesis states that there is no difference between the two groups. If the p-value is below a certain threshold (e.g. 0.05), we reject the null hypothesis and conclude that there is a significant difference between the two groups. The t-test assumes that both groups have the same variance, the welch t-test is a variation of the t-test that does not assume equal variances, and is more robust to violations of this assumption.

The t-test is only able to reject the null hypothesis, or fail to reject it. It does not provide any information about the size of the effect, or the probability of the effect size. This is a major limitation of the t-test, as it does not provide any information about the practical significance of the difference between the two groups.

## 1.3. A bayesian alternative

Bayesian or probabilistic alternatives to the t-test come with their own set of limitations. There is also no agreement on which is the best alternative, as there are a few approuches. Kruschke (2013) suggests a method termed BEST (bayesian estimation supersedes t test) that relies on the posterior distributions of the groups and their difference. It is able to provide more informative inference and is able to accept the null hypothesis. The decision is based on HDI and the concept of ROPE (region of practical equivalence). A high value for ROPE---a big proportion of the HDI is within the ROPE---indicates that there is no significant difference between the two groups. A low value conversly indicates that there is a significant difference. As these values are computed from the posterior distributions, they can provide a more nuanced view of the data. The BEST -method has its limitations as it is only really usable for two groups, and the model structure while flexible is complex and requires careful consideration of the priors. It can however be used for even small datasets, and is able to handle non-normality and heteroscedasticity (Kruschke, 2013).

Other alternatives have been proposed, which rely on the Bayes factor. They are in essance a model comparison, where the Bayes factor is used to compare the null model (no difference between the groups) to the alternative model (difference between the groups). The Bayes factor is a ratio of the posterior probabilities of the two models, and can be used to assess the strength of evidence for or against the null hypothesis (Kelter, 2021). 

The main benefit from Bayesian an probabilistic methods comes from the richer inference they provide. As Gelman et al. (2013) write, "In fact, most of the difficulties
in interpreting hypothesis tests arise from the artificial dichotomy that is required between
$\theta = \theta_0$ and $\theta \neq \theta_0$...for a continuous parameter $\theta$, the question ‚ÄòDoes $\theta$ equal 0?‚Äô can generally
be rephrased more usefully as ‚ÄòWhat is the posterior distribution for $\theta$?‚Äô" 

# 2. Case-study - Comparing alpha diversity between two groups

In this case-study we will analyze the alpha diversity differences across two geographical regions, Ahmednagar and Nashik. The data is included in the `mia` package and is a subset of the `peerj13075` dataset. This dataset contains skin microbial profiles, generated from 16S amplicon sequencing data.

## 2.1. Data preparation

```{r}
#| label: setup
#| echo: false
#| output: false 

# load libraries
library(mia)
library(scater)


# Load example dataset
data("peerj13075", package = "mia")
tse <- peerj13075
```

The `peerj13075` dataset contains skin microbial profiles from 58 volunteers. Data is from 16S amplicon sequencing and contains samples from across 3 regions. 

```{r}
#| label: data-wrangling 

# Selecting only two regions
tse <- tse[, tse$Geographical_location %in% c("Ahmednagar", "Nashik")]
dim(tse)
```

Keeping the samples from the regions of Ahmednagar and Nashik we retain `r dim(tse)[2]` samples.

```{r}
#| label: addAlpha

# Adding Shannon index
tse <- addAlpha(
    tse,
    assay.type = "counts",
    index = "shannon",
    detection = 10
)

# Plotting 
plotColData(
    tse,
    "shannon",
    "Geographical_location") +
    labs(
        x = "Geographical location",
        y = "Shannon index"
    ) +
    theme_bw()
```

After computing the Shannon index the plotted data shows that between the two regions there doesn't appear to be large difference, although within the samples from Ahmednagar there is a wider range of Shanon index values.

## 2.2. The t-test

To confirm with statistical testing whether the regions differ in Shannon index we can use a t-test. One of the assumptions of the t-test is the normality of the data. In the violin plot above the data appears to be normally distributed, or at least close enough. 

```{r}
#| label: t-test

# Performing t-test
tRes <- t.test(
    tse$shannon[tse$Geographical_location == "Ahmednagar"],
    tse$shannon[tse$Geographical_location == "Nashik"]
)

tRes
```

The t-test returns a p-value of `r format.pval(tRes$p.value, digits = 3)` with the confidence interval of `r paste0("(", round(tRes$conf.int[1], 2), ", ", round(tRes$conf.int[2], 2), ")")`. The t-test suggests that there is no significant difference in Shannon index between the two regions. 
We fail to reject the null hypothesis. 

## 2.3. A probabilistic alternative

As Kruschke (2013) points out in his paper, the probabilistic alternative to comparing two groups is able to provide more information, an intuitive interpretation, and a more robust framework. All without touching p-values, often misunderstood.

The approach Kruschke (2013) suggests---bayesian estimation supersedes t test (BEST)---is to compute the posterior distribution of the difference between the two groups. This is done by sampling from the posterior distribution of each group, and then computing the difference between them. The posterior distribution of the difference can then be used to compute credible intervals, and to assess whether the two groups differ.

The BEST model uses bayesian estimation to estimate 5 parameters in total. The mean and sd of each group, and a hyperparameter that controls the level of normality of the t-distribution. From the two distributions for the groups posterior draws can be made, and a posterior distribution for the difference computed. From this distribution the decision and inference can be made, based on HDI (highest density interval, e.g. 95%) and the area inside the ROPE (region of practical equivalence.) The ROPE must be defined beforehand based on domain knowledge. What is the region of practical equivalence? What is the smallest difference that is considered meaningful? In this example we will use a ROPE of [-0.2, 0.2], meaning that any difference between the two groups that is within this range is considered practically equivalent. This is the crux of the problem, since defining and justifying the ROPE is often difficult.

Setting the priors is a crucial step, here relatively uninformative priors have been chosen. Note the prior on group means is a truncated normal distribution, which is centered around 3. A typical range of Shannon index values in human microbiomes is between 1-4.


```{r}
#| label: libraries
#| output: false

# Load libraries
library(brms)
library(bayesplot)
library(bayestestR)
library(dplyr)
library(posterior)
```

```{r}
#| label: bayes-test
#| output: false

# Prepare date
df <- as.data.frame(colData(tse))
df <- df %>%
    select(Geographical_location, shannon) %>%
    mutate(Geographical_location = factor(Geographical_location)) %>%
    filter(Geographical_location %in% c("Ahmednagar", "Nashik"))

# BEST Method Implementation
# Using brm with heteroskedastic model
fit_best <- brm(
  bf(shannon ~ 0 + Geographical_location,  # No intercept, estimate group means directly
     sigma ~ 0 + Geographical_location),   # Allow different variances per group
  data = df,
  family = student(),
  prior = c(
    # Priors on group means, truncated to be positive
    prior(normal(3, 1), class = "b", dpar = "", lb = 0, ub = Inf),
    # Priors on log(sigma) for each group  
    prior(normal(0, 1), class = "b", dpar = "sigma"),
    # Prior on degrees of freedom (nu) - controls heaviness of tails
    prior(gamma(2, 0.1), class = "nu")
  ),
  chains = 8,
  iter = 4000,
  cores = 8,
  seed = 42
)
```

```{r}
#| label: best-analysis

# Get posterior draws
draws <- as_draws_df(fit_best)

# Extract group means
mu_ahmednagar <- draws$b_Geographical_locationAhmednagar
mu_nashik <- draws$b_Geographical_locationNashik

# Extract group standard deviations (note: brms uses log link for sigma)
sigma_ahmednagar <- exp(draws$b_sigma_Geographical_locationAhmednagar)
sigma_nashik <- exp(draws$b_sigma_Geographical_locationNashik)

# Compute the difference (key BEST parameter)
difference <- mu_ahmednagar - mu_nashik

# BEST Summary Statistics
output <- capture.output({
    cat("BEST Analysis Results:\n")
    cat("====================\n\n")

    # Group means
    cat("Group Means:\n")
    cat("Ahmednagar: ", round(mean(mu_ahmednagar), 3), " [", 
    round(quantile(mu_ahmednagar, 0.025), 3), ", ", 
    round(quantile(mu_ahmednagar, 0.975), 3), "]\n")
    cat("Nashik: ", round(mean(mu_nashik), 3), " [", 
    round(quantile(mu_nashik, 0.025), 3), ", ", 
    round(quantile(mu_nashik, 0.975), 3), "]\n\n")

    # Group standard deviations
    cat("Group Standard Deviations:\n")
    cat("Ahmednagar: ", round(mean(sigma_ahmednagar), 3), " [", 
    round(quantile(sigma_ahmednagar, 0.025), 3), ", ", 
    round(quantile(sigma_ahmednagar, 0.975), 3), "]\n")
    cat("Nashik: ", round(mean(sigma_nashik), 3), " [", 
    round(quantile(sigma_nashik, 0.025), 3), ", ", 
    round(quantile(sigma_nashik, 0.975), 3), "]\n\n")

    # Difference between groups
    cat("Difference (Ahmednagar - Nashik):\n")
    cat("Mean difference: ", round(mean(difference), 3), "\n")
    cat("95% HDI: [", round(quantile(difference, 0.025), 3), ", ", 
    round(quantile(difference, 0.975), 3), "]\n")

    # Probability that Ahmednagar > Nashik
    prob_greater <- mean(difference > 0)
    cat("P(Ahmednagar > Nashik): ", round(prob_greater, 3), "\n")
})
cat(output, sep = "\n")
```

```{r}
#| label: best-rope

# ROPE Analysis (Region of Practical Equivalence)
rope_lower <- -0.2
rope_upper <- 0.2

# Calculate 95% HDI first
hdi_result <- hdi(difference, ci = 0.95)
hdi_lower <- hdi_result$CI_low
hdi_upper <- hdi_result$CI_high

output <- capture.output({
    cat("\nROPE Analysis (HDI-based):\n")
    cat("==========================\n")
    cat("ROPE bounds: [", rope_lower, ", ", rope_upper, "]\n")
    cat("95% HDI: [", round(hdi_lower, 3), ", ", round(hdi_upper, 3), "]\n\n")
})
cat(output, sep = "\n")

# Calculate HDI-ROPE overlap proportion
# Extract only the posterior samples that fall within the 95% HDI
hdi_samples <- difference[difference >= hdi_lower & difference <= hdi_upper]
# Calculate what proportion of these HDI samples also fall within ROPE
hdi_rope_overlap_proportion <- mean(hdi_samples >= rope_lower & hdi_samples <= rope_upper)

# Proportion of full posterior in ROPE (for comparison)
full_rope_proportion <- mean(difference >= rope_lower & difference <= rope_upper)

# ROPE Decision based on HDI
output <- capture.output({
    cat("HDI-ROPE overlap proportion: ", round(hdi_rope_overlap_proportion, 3), "\n")
    cat("Full posterior in ROPE: ", round(full_rope_proportion, 3), "\n\n")

    # Kruschke's decision rules based on HDI-ROPE relationship
    if (hdi_rope_overlap_proportion > 0.95) {
    decision <- "Accept practical equivalence (>95% of HDI within ROPE)"
    } else if (hdi_rope_overlap_proportion < 0.05) {
    decision <- "Reject practical equivalence (<5% of HDI within ROPE)"
    } else {
    decision <- "Inconclusive (HDI substantially overlaps ROPE boundary)"
    }

    cat("HDI-based Decision: ", decision, "\n")
})
cat(output, sep = "\n")
```

```{r}
#| label: best-plots
#| fig-height: 8
#| fig-width: 12

# BEST Visualizations
library(ggplot2)
library(patchwork)

# Create data frame for plotting
plot_data <- data.frame(
  difference = difference,
  mu_ahmednagar = mu_ahmednagar,
  mu_nashik = mu_nashik,
  sigma_ahmednagar = sigma_ahmednagar,
  sigma_nashik = sigma_nashik
)

# Plot 1: Posterior distribution of the difference with HDI
p1 <- ggplot(plot_data, aes(x = difference)) +
  geom_density(fill = "cornflowerblue", alpha = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  geom_vline(xintercept = rope_lower, linetype = "dotted", color = "green") +
  geom_vline(xintercept = rope_upper, linetype = "dotted", color = "green") +
  geom_vline(xintercept = hdi_lower, linetype = "dashed", color = "blue", linewidth = 1) +
  geom_vline(xintercept = hdi_upper, linetype = "dashed", color = "blue", linewidth = 1) +
  annotate("rect", xmin = rope_lower, xmax = rope_upper, 
           ymin = -Inf, ymax = Inf, alpha = 0.3, fill = "lightgreen") +
  annotate("rect", xmin = hdi_lower, xmax = hdi_upper, 
           ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "steelblue") +
  labs(title = "Posterior Distribution with HDI and ROPE",
       subtitle = "Red: no difference | Green: ROPE | Blue: 95% HDI",
       x = "Shannon Index Difference (Ahmednagar - Nashik)",
       y = "Density") +
  theme_minimal()

# Plot 2: Group means
p2 <- ggplot() +
  geom_density(data = plot_data, aes(x = mu_ahmednagar), 
               fill = "orange", alpha = 0.7, color = "orange") +
  geom_density(data = plot_data, aes(x = mu_nashik), 
               fill = "purple", alpha = 0.7, color = "purple") +
  labs(title = "Posterior Distributions of Group Means",
       x = "Shannon Index",
       y = "Density") +
  annotate("text", x = max(c(mu_ahmednagar, mu_nashik)), y = Inf, 
           label = "Orange: Ahmednagar\nPurple: Nashik", 
           vjust = 2, hjust = 1) +
  theme_minimal()

# Plot 3: Group standard deviations
p3 <- ggplot() +
  geom_density(data = plot_data, aes(x = sigma_ahmednagar), 
               fill = "orange", alpha = 0.7, color = "orange") +
  geom_density(data = plot_data, aes(x = sigma_nashik), 
               fill = "purple", alpha = 0.7, color = "purple") +
  labs(title = "Posterior Distributions of Group SDs",
       x = "Shannon Index Standard Deviation",
       y = "Density") +
  theme_minimal()

# Plot 4: Effect size (Cohen's d equivalent)
# Group sizes are equal, so we can use pooled SD, without weighting
pooled_sd <- sqrt((mean(sigma_ahmednagar^2) + mean(sigma_nashik^2)) / 2)
cohens_d <- difference / pooled_sd
p4 <- ggplot(data.frame(cohens_d = cohens_d), aes(x = cohens_d)) +
  geom_density(fill = "lightgreen", alpha = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  geom_vline(xintercept = c(-0.2, 0.2), linetype = "dotted", color = "gray") +
  labs(title = "Posterior Distribution of Effect Size",
       subtitle = "Gray lines: small effect boundaries (¬±0.2)",
       x = "Cohen's d",
       y = "Density") +
  theme_minimal()

# Combine plots
(p1 + p2) / (p3 + p4)
```

Here a large portion of the HDI resides within the ROPE, indicating that there is no significant difference between the two groups. However, the method does inform us that the Ahmednagar region has a substantial probability of having a higher mean Shannon index then the Nashik region. 

# 2.4. Conclusions on BEST

In this case-study we have demonstrated how to use a probabilistic alternative to the two sample t-test, the BEST -method, to compare alpha diversity between two groups. The BEST -method is able to provide more information than the t-test, and is able to handle non-normality and heteroscedasticity. It is also able to accept the null hypothesis, and provide a more nuanced view of the data. However as Kelter (2021) points out both approuches to testing have their limitations and nuances. Bayesian or probabilistic methods have stronger control for type I error, and provide flexible modeling options with richer inference, at the cost of complexity and higher type II error. In contrast frequentist methods remain straightforward, entrenched in tradition, and control better for type II error (Kelter, 2021).

# 3. Taking a step back

Vuorre (2017) presents the idea of thinking of t-tests as linear models, which naturally allows the extension to more complicated situation.
This is a useful perspective, as it motivates the rational of using Bayesian estimation as a tool for group comparisons. 

In the simplest case, comparing two groups can be formulated as a two class linear model. 

$$ y_{ik} \sim \mathcal{N}(\mu_k, \sigma_k^2) $$

where $y_{ik}$ is the $i$th observation from group $k$, $\mu_k$ is the mean of group $k$, and $\sigma_k^2$ is the variance of group $k$.

$$ \mu_k = \beta_0 + \beta_k $$

where $\beta_0$ is the intercept, and $\beta_k$ is the effect of group $k$. 

Similarly, the variance can be modeled as:

$$ \sigma_k^2 = \gamma_0 + \gamma_k $$

where $\gamma_0$ is the intercept for the variance, and $\gamma_k$ is the effect of group $k$ on the variance.

This would correspond to the Welch T-test, where groups are allowed to have different variances. Unlike the BEST -method, this models the data with a Normal distribution, and does not use a t-distribution. No need to estimate the $\nu$ parameter, which controls the heaviness of the tails.

## 3.1. Fitting a linear model 

Using the `brms` package and the same data as above.

```{r}
#| label: linear-model

mod <- brm(
    bf(
        shannon ~ Geographical_location,
        sigma ~ Geographical_location
    ),
    data = df,
    family = gaussian(),
    cores = 8,
    chains = 8,
    iter = 4000
)

summary(mod)
```

Interpreting the model summary, we can first check the ESS (Effective sample size) values are sufficiently high. A general rule of thumb is that ESS should be at least 1000 for each parameter being estimated to ensure reliable estimates. Going much further than that is often not necessary as diminishing returns start to kick in. Rhat values should be 1, indicating that chains have converged. Small deviance form 1 should warrent further inspection of the chains and model fit. 

With the model formulation, our baseline is the Ahmednagar region. The intercept's posterior distribution is the mean Shannon index for Ahmednagar, and the coefficient for Nashik is the difference in mean Shannon index between the two regions. Likewise the sigma_Intercept is the baseline variance for Ahmednagar, and the coefficient for Nashik is the difference in variance between the two regions. This is contrast to the ealier BEST -method where the model was formulated to estimate the means and variances of each group separately without an intercept. 

## 3.2. Plotting the posterior distributions of the means and variances.

We can plot the posterior distributions in a similar way as we did with the BEST -method.

```{r}
#| label: linear-model-plots
#| fig-height: 4
#| fig-width: 12

# Extract posterior draws
draws <- as_draws_df(mod)
mu_ahmednagar <- draws$b_Intercept
mu_nashik <- draws$b_Geographical_locationNashik + mu_ahmednagar
sigma_ahmednagar <- exp(draws$b_sigma_Intercept)
sigma_nashik <- exp(draws$b_sigma_Geographical_locationNashik + draws$b_sigma_Intercept)

# Create data frame for plotting
plot_data <- data.frame(
    mu_nashik = mu_nashik,
    mu_ahmednagar = mu_ahmednagar,
    sigma_nashik = sigma_nashik,
    sigma_ahmednagar = sigma_ahmednagar
)

# Plot 1: Posterior distributions of group means
p1 <- ggplot(plot_data,) +
    geom_density(aes(x = mu_ahmednagar), fill = "orange", alpha = 0.7) +
    geom_density(aes(x = mu_nashik), fill = "purple", alpha = 0.7) +
    labs(title = "Posterior Distributions of Group Means",
         x = "Shannon Index",
         y = "Density") +
    annotate("text", x = max(c(mu_nashik, mu_ahmednagar)), y = Inf, 
             label = "Orange: Ahmednagar\nPurple: Nashik", 
             vjust = 2, hjust = 1) +
    theme_minimal() 

# Plot 2: Posterior distributions of group standard deviations
p2 <- ggplot(plot_data) +
    geom_density(aes(x = sigma_ahmednagar), fill = "orange", alpha = 0.7) +
    geom_density(aes(x = sigma_nashik), fill = "purple", alpha = 0.7) +
    labs(title = "Posterior Distributions of Group Standard Deviations",
         x = "Shannon Index Standard Deviation",
         y = "Density") +
    theme_minimal()

# Combine plots
p1 + p2
```

The posterior distributions of the means and standard deviations are similar to the BEST -method results. Notice that the shape of distributions is less smooth, most likely due to lower ESS, and slight difference in the model formulation using an gaussian distribution instead of a t-distribution.

Just looking at these two plots, we can see that the regions appear to differ, in both means and variances. Is the difference 'significant'? Are we even interested in putting a binary label on it?

## 3.3. Making it Robust

In the above model the response, shannon diversity, is modeled with a Normal distribution. In the earlier case-study using the BEST -method the response was modeled with a t-distribution. The t-distribution has heavier tails, meaning more probability mass is distributed to the extreme ends of the distribution. The extent of which is controlled by the hyperparameter $\nu$.
When $\nu$ approaches infinity the t-distribution is practically the normal the distribution. The smaller the value of $\nu$ the more mass is at the tails. The BEST-method pooled the data and learned a shared $\nu$.

Substituting in the t-distribution and adding the estimation of $\nu$ to the model is a logical next step. This way the model is better able to handle outliers or extremes.

```{r}
#| label: robust-linear-model

modRob <- brm(
    bf(
        shannon ~ Geographical_location,
        sigma ~ Geographical_location
    ),
    data = df,
    family = student(),
    cores = 8,
    chains = 8,
    iter = 4000
)

summary(modRob)
```

```{r}
#| label: robust-linear-model-plots
#| fig-height: 4
#| fig-width: 12

# Extract posterior draws
draws <- as_draws_df(modRob)
mu_ahmednagar <- draws$b_Intercept
mu_nashik <- draws$b_Geographical_locationNashik + mu_ahmednagar
sigma_ahmednagar <- exp(draws$b_sigma_Intercept)
sigma_nashik <- exp(draws$b_sigma_Geographical_locationNashik + draws$b_sigma_Intercept)

# Create data frame for plotting
plot_data <- data.frame(
    mu_nashik = mu_nashik,
    mu_ahmednagar = mu_ahmednagar,
    sigma_nashik = sigma_nashik,
    sigma_ahmednagar = sigma_ahmednagar
)

# Plot 1: Posterior distributions of group means
p1 <- ggplot(plot_data,) +
    geom_density(aes(x = mu_ahmednagar), fill = "orange", alpha = 0.7) +
    geom_density(aes(x = mu_nashik), fill = "purple", alpha = 0.7) +
    labs(title = "Posterior Distributions of Group Means",
         x = "Shannon Index",
         y = "Density") +
    annotate("text", x = max(c(mu_nashik, mu_ahmednagar)), y = Inf, 
             label = "Orange: Ahmednagar\nPurple: Nashik", 
             vjust = 2, hjust = 1) +
    theme_minimal() 

# Plot 2: Posterior distributions of group standard deviations
p2 <- ggplot(plot_data) +
    geom_density(aes(x = sigma_ahmednagar), fill = "orange", alpha = 0.7) +
    geom_density(aes(x = sigma_nashik), fill = "purple", alpha = 0.7) +
    labs(title = "Posterior Distributions of Group Standard Deviations",
         x = "Shannon Index Standard Deviation",
         y = "Density") +
    theme_minimal()

# Combine plots
p1 + p2
```

The posterior distributions look about the same as with the normal model. Closer inspection of the model summary printouts below, reveals that the t-distribution model has higher effective sample sizes (ESS) which would suggest better sampling efficiency. Rhats are 1 across the board, indicating that the chains have converged. Otherwise the results and credible intervals are similar between the two models. The advantage of the 'robust' model is theoretical in this case, as the data seems to fit well to the normal distribution. 

```{r}
#| label: robust-model-summary

summary(mod)

summary(modRob)
```





# Bibliography

- Kruschke, J. K. (2013). Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General, 142(2), 573‚Äì603. https://doi.org/10.1037/a0029146

- Kelter, R. (2021). Bayesian and frequentist testing for differences between two groups with parametric and nonparametric two‚Äêsample tests. WIREs Computational Statistics, 13(6). https://doi.org/10.1002/wics.1523

- Gelman, A., Carlin, J.‚ÄØB., Stern, H.‚ÄØS., Dunson, D.‚ÄØB., Vehtari, A., & Rubin, D.‚ÄØB. (2013). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC. https://doi.org/10.1201/b16018

- Vuorre, Matti. 2017. ‚ÄúHow to Compare Two Groups with Robust Bayesian Estimation in R.‚Äù January 2, 2017. [https://vuorre.com/posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/](https://vuorre.com/posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/).








